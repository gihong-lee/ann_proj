{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU found\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "else:\n",
    "    print(\"No GPU found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_path = \"../data\"\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(f'{data_dir_path}/emnist-byclass-train.csv', header=None)\n",
    "test_df = pd.read_csv(f\"{data_dir_path}/emnist-byclass-test.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_csv_to_numpy(data, sorting = False): # pandas 통해 읽은 csv data numpy 형태로 변경\n",
    "  if sorting == True:\n",
    "    data = data.sort_values(by=[0], axis=0)\n",
    "\n",
    "  label = np.array(data[0]) # csv file 에서 0번째 colum은 index임\n",
    "  only_data = np.array(data.drop([0], axis = 1)).reshape((-1, 28, 28, 1)) # csv file에서 0번 째 colum 탈락 -> data만 남게 됨\n",
    "  # only_data = tf.convert_to_tensor(only_data, dtype=tf.float32)\n",
    "  return only_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train  = convert_data_csv_to_numpy(train_df)\n",
    "X_test, y_test  = convert_data_csv_to_numpy(test_df)\n",
    "\n",
    "train_df = None\n",
    "test_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "X_train = np.repeat(X_train, 3, axis=-1)\n",
    "X_test = np.repeat(X_test, 3, axis=-1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# X_train = tf.image.resize(X_train, [32,32])\n",
    "# X_val = tf.image.resize(X_val, [32,32])\n",
    "# X_test = tf.image.resize(X_test, [32,32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(558345, 28, 28, 3) (139587, 28, 28, 3) (116323, 28, 28, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # expand new axis, channel axis \n",
    "# X_train = np.expand_dims(X_train, axis=-1)\n",
    "# X_val = np.expand_dims(X_val, axis=-1)\n",
    "# X_test = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "# # [optional]: we may need 3 channel (instead of 1)\n",
    "# X_train = np.repeat(X_train, 3, axis=-1)\n",
    "# X_val = np.repeat(X_val, 3, axis=-1)\n",
    "# X_test = np.repeat(X_test, 3, axis=-1)\n",
    "\n",
    "# # it's always better to normalize \n",
    "# X_train = X_train.astype('float32') / 255\n",
    "# X_val = X_val.astype('float32') / 255\n",
    "# X_test = X_test.astype('float32') / 255\n",
    "\n",
    "# # resize the input shape , i.e. old shape: 28, new shape: 32\n",
    "# X_train = tf.image.resize(X_train, [32,32]) # if we want to resize \n",
    "# X_val = tf.image.resize(X_val, [32,32]) # if we want to resize \n",
    "# X_test = tf.image.resize(X_test, [32,32]) # if we want to resize \n",
    "\n",
    "# # one hot \n",
    "# y_train = tf.keras.utils.to_categorical(y_train , num_classes=10)\n",
    "# y_val = tf.keras.utils.to_categorical(y_val , num_classes=10)\n",
    "# y_test = tf.keras.utils.to_categorical(y_test , num_classes=10)\n",
    "\n",
    "# print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(keras.models.Model):\n",
    "    \"\"\"\n",
    "    A standard resnet block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels: int, down_sample=False):\n",
    "        \"\"\"\n",
    "        channels: same as number of convolution kernels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.__channels = channels\n",
    "        self.__down_sample = down_sample\n",
    "        self.__strides = [2, 1] if down_sample else [1, 1]\n",
    "\n",
    "        KERNEL_SIZE = (3, 3)\n",
    "        # use He initialization, instead of Xavier (a.k.a 'glorot_uniform' in Keras), as suggested in [2]\n",
    "        INIT_SCHEME = \"he_normal\"\n",
    "\n",
    "        self.conv_1 = keras.layers.Conv2D(self.__channels, strides=self.__strides[0],\n",
    "                             kernel_size=KERNEL_SIZE, padding=\"same\", kernel_initializer=INIT_SCHEME)\n",
    "        self.bn_1 = keras.layers.BatchNormalization()\n",
    "        self.conv_2 = keras.layers.Conv2D(self.__channels, strides=self.__strides[1],\n",
    "                             kernel_size=KERNEL_SIZE, padding=\"same\", kernel_initializer=INIT_SCHEME)\n",
    "        self.bn_2 = keras.layers.BatchNormalization()\n",
    "        self.merge = keras.layers.Add()\n",
    "\n",
    "        if self.__down_sample:\n",
    "            # perform down sampling using stride of 2, according to [1].\n",
    "            self.res_conv = keras.layers.Conv2D(\n",
    "                self.__channels, strides=2, kernel_size=(1, 1), kernel_initializer=INIT_SCHEME, padding=\"same\")\n",
    "            self.res_bn = keras.layers.BatchNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        res = inputs\n",
    "\n",
    "        x = self.conv_1(inputs)\n",
    "        x = self.bn_1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.bn_2(x)\n",
    "\n",
    "        if self.__down_sample:\n",
    "            res = self.res_conv(res)\n",
    "            res = self.res_bn(res)\n",
    "\n",
    "        # if not perform down sample, then add a shortcut directly\n",
    "        x = self.merge([x, res])\n",
    "        out = tf.nn.relu(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet18(keras.models.Model):\n",
    "\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        \"\"\"\n",
    "            num_classes: number of classes in specific classification task.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv_1 = keras.layers.Conv2D(64, (7, 7), strides=2,\n",
    "                             padding=\"same\", kernel_initializer=\"he_normal\")\n",
    "        self.init_bn = keras.layers.BatchNormalization()\n",
    "        self.pool_2 = keras.layers.MaxPool2D(pool_size=(2, 2), strides=2, padding=\"same\")\n",
    "        self.res_1_1 = ResnetBlock(64)\n",
    "        self.res_1_2 = ResnetBlock(64)\n",
    "        self.res_2_1 = ResnetBlock(128, down_sample=True)\n",
    "        self.res_2_2 = ResnetBlock(128)\n",
    "        self.res_3_1 = ResnetBlock(256, down_sample=True)\n",
    "        self.res_3_2 = ResnetBlock(256)\n",
    "        self.res_4_1 = ResnetBlock(512, down_sample=True)\n",
    "        self.res_4_2 = ResnetBlock(512)\n",
    "        self.avg_pool = keras.layers.GlobalAveragePooling2D()\n",
    "        self.flat = keras.layers.Flatten()\n",
    "        self.fc = keras.layers.Dense(num_classes, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        out = self.conv_1(inputs)\n",
    "        out = self.init_bn(out)\n",
    "        out = tf.nn.relu(out)\n",
    "        out = self.pool_2(out)\n",
    "        for res_block in [self.res_1_1, self.res_1_2, self.res_2_1, self.res_2_2, self.res_3_1, self.res_3_2, self.res_4_1, self.res_4_2]:\n",
    "            out = res_block(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = self.flat(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "resnet_model = ResNet18(62)\n",
    "resnet_model.build(input_shape = (None,28,28,1))\n",
    "#use categorical_crossentropy since the label is one-hot encoded\n",
    "\n",
    "# from keras.optimizers import SGD\n",
    "# opt = SGD(learning_rate=0.1,momentum=0.9,decay = 1e-04) #parameters suggested by He [1]\n",
    "resnet_model.compile(optimizer = \"adam\",loss='categorical_crossentropy', metrics=[\"accuracy\"]) \n",
    "resnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = tf.keras.Input(shape=(32,32,3))\n",
    "# efnet = tf.keras.applications.ResNet50(weights='imagenet',\n",
    "#                                              include_top = False, \n",
    "#                                              input_tensor = input)\n",
    "# # Now that we apply global max pooling.\n",
    "\n",
    "# # Finally, we add a classification layer.\n",
    "# number_output = tf.keras.layers.Dense(10, activation='softmax', use_bias=True)()\n",
    "# upper_output = tf.keras.layers.Dense(26,  activation='softmax', use_bias=True)()\n",
    "# lower_output = tf.keras.layers.Dense(26,  activation='softmax', use_bias=True)()\n",
    "\n",
    "# # bind all\n",
    "# func_model = tf.keras.Model(efnet.input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "aug = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\",\n",
    ")\n",
    "\n",
    "es = EarlyStopping(patience= 8, restore_best_weights=True)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"new_model.h5\", save_best_only=True)\n",
    "\n",
    "batch_size = 256\n",
    "#I did not use cross validation, so the validate performance is not accurate.\n",
    "STEPS = len(X_train) / 256\n",
    "\n",
    "history = resnet_model.fit(aug.flow(X_train,y_train,batch_size = batch_size), \n",
    "      steps_per_epoch=STEPS, \n",
    "      batch_size = batch_size, \n",
    "      epochs=50, \n",
    "      validation_data=(X_train, y_train),\n",
    "      callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.keras.layers.Resizing(height = 32, width = 32, input_shape = [28, 28, 3])\n",
    "\n",
    "input = tf.keras.Input(shape=(32,32,3))\n",
    "efnet = tf.keras.applications.ResNet50(weights='imagenet',\n",
    "                                             include_top = False, \n",
    "                                             input_tensor = input)\n",
    "# Now that we apply global max pooling.\n",
    "gap = tf.keras.layers.GlobalMaxPooling2D()\n",
    "\n",
    "# Finally, we add a classification layer.\n",
    "output = tf.keras.layers.Dense(62, activation='softmax', use_bias=True)\n",
    "\n",
    "# bind all\n",
    "func_model = keras.models.Sequential()\n",
    "func_model.add(input_layer)\n",
    "func_model.add(efnet)\n",
    "func_model.add(gap)\n",
    "func_model.add(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resizing_2 (Resizing)       (None, 32, 32, 3)         0         \n",
      "                                                                 \n",
      " resnet50 (Functional)       (None, 1, 1, 2048)        23587712  \n",
      "                                                                 \n",
      " global_max_pooling2d_2 (Glo  (None, 2048)             0         \n",
      " balMaxPooling2D)                                                \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 62)                127038    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,714,750\n",
      "Trainable params: 23,661,630\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "func_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid keyword argument(s) in `compile()`: ({'callbacks'},). Valid keyword arguments include \"cloning\", \"experimental_run_tf_function\", \"distribute\", \"target_tensors\", or \"sample_weight_mode\".",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m es \u001b[38;5;241m=\u001b[39m EarlyStopping(patience\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# func_model.compile(\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#           loss  = 'sparse_categorical_crossentropy',\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#           metrics = tf.keras.metrics.SparseCategoricalAccuracy(),\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#           optimizer = tf.keras.optimizers.Adam())\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43mfunc_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m          \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msparse_categorical_crossentropy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres50_checkpoint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\anaconda\\envs\\ann\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\anaconda\\envs\\ann\\lib\\site-packages\\keras\\engine\\training.py:2786\u001b[0m, in \u001b[0;36mModel._validate_compile\u001b[1;34m(self, optimizer, metrics, **kwargs)\u001b[0m\n\u001b[0;32m   2784\u001b[0m invalid_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(kwargs) \u001b[38;5;241m-\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_weight_mode\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m   2785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m invalid_kwargs:\n\u001b[1;32m-> 2786\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid keyword argument(s) in `compile()`: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2787\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(invalid_kwargs,)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Valid keyword arguments include \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2788\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcloning\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperimental_run_tf_function\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistribute\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2789\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, or \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   2791\u001b[0m \u001b[38;5;66;03m# Model must be created and compiled with the same DistStrat.\u001b[39;00m\n\u001b[0;32m   2792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;129;01mand\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mhas_strategy():\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid keyword argument(s) in `compile()`: ({'callbacks'},). Valid keyword arguments include \"cloning\", \"experimental_run_tf_function\", \"distribute\", \"target_tensors\", or \"sample_weight_mode\"."
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "res50_checkpoint = keras.callbacks.ModelCheckpoint(\"res50.h5\", save_best_only=True)\n",
    "es = EarlyStopping(patience= 8, restore_best_weights=True)\n",
    "\n",
    "# func_model.compile(\n",
    "#           loss  = 'sparse_categorical_crossentropy',\n",
    "#           metrics = tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "#           optimizer = tf.keras.optimizers.Adam())\n",
    "\n",
    "func_model.compile(optimizer='adam',\n",
    "          loss='sparse_categorical_crossentropy', \n",
    "          metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_model.fit(X_train, y_train, epochs=5, verbose = 2,\n",
    "          callbacks=[es, res50_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1cca9558bc5ad879ec93cc030b157d75f18267527c60932cecaace349eef54dd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
