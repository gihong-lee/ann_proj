{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import datasets, layers, models, losses, Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout, Input, BatchNormalization, Dropout\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "else:\n",
    "    print(\"No GPU found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_csv_to_numpy(data, sorting = False): # pandas 통해 읽은 csv data numpy 형태로 변경\n",
    "  if sorting == True:\n",
    "    data = data.sort_values(by=[0], axis=0)\n",
    "\n",
    "  label = np.array(data[0]) # csv file 에서 0번째 colum은 index임\n",
    "  only_data = np.array(data.drop([0], axis = 1)) # csv file에서 0번 째 colum 탈락 -> data만 남게 됨\n",
    "  # only_data = tf.convert_to_tensor(only_data, dtype=tf.float32)\n",
    "  only_data = tf.keras.utils.normalize(only_data, axis=-1, order=2).reshape((-1, 28, 28, 1)) #data nomalize & reshaping\n",
    "  return only_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_path = \"../data\"\n",
    "\n",
    "train_df = pd.read_csv(f'{data_dir_path}/emnist-byclass-train.csv', header=None)\n",
    "test_df = pd.read_csv(f\"{data_dir_path}/emnist-byclass-test.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train  = convert_data_csv_to_numpy(train_df)\n",
    "X_test, y_test  = convert_data_csv_to_numpy(test_df)\n",
    "\n",
    "train_df = None\n",
    "test_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add channel\n",
    "# channel = 3\n",
    "# X_train_3 = np.repeat(X_train, channel, axis=-1)\n",
    "# X_test_3 = np.repeat(X_test, channel, axis=-1)\n",
    "# y_test_3 = y_test\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "# X_train_3, X_val_3, y_train_3, y_val_3 = train_test_split(X_train_3, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data resizing\n",
    "# X_train = tf.image.resize(X_train, [32,32])\n",
    "# X_val = tf.image.resize(X_val, [32,32])\n",
    "# X_test = tf.image.resize(X_test, [32,32])\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_val = tf.keras.utils.to_categorical(y_val)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape,y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data_scheme(data, label, row = 4, col = 5, i = 1): # data 어떻게 생겼는지 plot\n",
    "  label_value_list = []\n",
    "  for i in range(62):\n",
    "    if i <=9:\n",
    "      label_value_list.append(f'{i}')\n",
    "    elif 10<=i<=35:\n",
    "      label_value_list.append(f'{chr(i+55)}')\n",
    "    else:\n",
    "      label_value_list.append(f'{chr(i+61)}')\n",
    "\n",
    "  plt.figure(figsize = (11, 8))\n",
    "  for r in range(row):\n",
    "    for c in range(col):\n",
    "      plt.subplot(row, col, i)\n",
    "      plt.imshow(data[i,...,0], 'gray')\n",
    "      plt.axis('off')\n",
    "      plt.title(f'{label_value_list[label[i]]}')\n",
    "      i+=1\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_data_scheme(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Base line accuracy 측정 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - A. Lenet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_5_model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(6, kernel_size=5, strides=1,  activation='tanh', input_shape=X_train[0].shape, padding='same'),\n",
    "    keras.layers.AveragePooling2D(),\n",
    "    keras.layers.Conv2D(16, kernel_size=5, strides=1, activation='tanh', padding='valid'),\n",
    "    keras.layers.AveragePooling2D(),\n",
    "    keras.layers.Conv2D(120, kernel_size=5, strides=1, activation='tanh', padding='valid'),\n",
    "    keras.layers.Flatten(),   \n",
    "    keras.layers.Dense(84, activation='tanh'),\n",
    "    keras.layers.Dense(62, activation='softmax')\n",
    "])\n",
    "lenet_5_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_5_model.compile(optimizer='adam', loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "# model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_lenet5_emnist_model.h5\", save_best_only=True)\n",
    "\n",
    "lenet_5_model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), callbacks=[early_stopping_cb])\n",
    "\n",
    "lenet_5_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet_5_SeLU\n",
    "\n",
    "SeLU performs better(speed & accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "lenet_5_selu_model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(6, kernel_size=5, strides=1,  activation=\"selu\", kernel_initializer=\"lecun_normal\", input_shape=X_train[0].shape, padding='same'),\n",
    "    keras.layers.AveragePooling2D(),\n",
    "    keras.layers.Conv2D(16, kernel_size=5, strides=1, activation=\"selu\", kernel_initializer=\"lecun_normal\", padding='valid'),\n",
    "    keras.layers.AveragePooling2D(),\n",
    "    keras.layers.Conv2D(120, kernel_size=5, strides=1, activation=\"selu\", kernel_initializer=\"lecun_normal\", padding='valid'),\n",
    "    keras.layers.Flatten(),   \n",
    "    keras.layers.Dense(84, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(62, activation='softmax')\n",
    "])\n",
    "\n",
    "lenet_5_selu_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_5_selu_model.compile(optimizer='adam', loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "# model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_lenet5_emnist_model.h5\", save_best_only=True)\n",
    "\n",
    "lenet_5_selu_model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), callbacks=[early_stopping_cb])\n",
    "\n",
    "lenet_5_selu_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - B. Mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile2 = tf.keras.applications.mobilenet_v2.MobileNetV2(input_shape = (224, 224, 3), include_top=False)\n",
    "\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "prediction_layer = tf.keras.layers.Dense(62, activation = 'softmax')\n",
    "\n",
    "mobile2_model = keras.models.Sequential()\n",
    "mobile2_model.add(tf.keras.layers.Normalization( axis=-1, mean=44.412914, variance=84.77896, input_shape = (28, 28, 3)))\n",
    "mobile2_model.add(tf.keras.layers.Resizing(height = 224, width = 224))\n",
    "mobile2_model.add(mobile2)\n",
    "mobile2_model.add(global_average_layer)\n",
    "mobile2_model.add(tf.keras.layers.Dense(1000))\n",
    "mobile2_model.add(tf.keras.layers.ReLU())\n",
    "mobile2_model.add(prediction_layer)\n",
    "\n",
    "mobile2_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile2_model.compile(loss=\"categorical_crossentropy\",optimizer=\"Adam\",steps_per_execution=1200,metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossAndErrorPrintingCallback(keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(\n",
    "            f'''\n",
    "            epcoh : {epoch}\n",
    "            loss {logs['loss']}\n",
    "            accuracy {logs['accuracy']}\n",
    "            val_loss {logs['val_loss']}\n",
    "            val_accuracy {logs['val_accuracy']}\n",
    "            '''\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  with tf.device(\"/cpu:0\"):\n",
    "mobile2_model_history = mobile2_model.fit(final_train[:400], train_label[:400], \n",
    "                  epochs=10, validation_data=(final_val[:80], val_label[:80]), callbacks=[LossAndErrorPrintingCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Model 선정 및 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 - A.  Lenet base - new architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "lenet_5_improved_model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(32, kernel_size=5, strides=2, activation=\"selu\", kernel_initializer=\"lecun_normal\", input_shape=X_train[0].shape, padding='same'),\n",
    "    keras.layers.Conv2D(64, kernel_size=3, strides=2, activation=\"selu\", kernel_initializer=\"lecun_normal\", padding='valid'),\n",
    "    keras.layers.Conv2D(32, kernel_size=1, strides=1, activation=\"selu\", kernel_initializer=\"lecun_normal\", padding='valid'),\n",
    "    keras.layers.GlobalAveragePooling2D(),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(62, activation='softmax')\n",
    "])\n",
    "\n",
    "lenet_5_improved_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_5_improved_model.compile(optimizer='adam', loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "# model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_lenet5_emnist_model.h5\", save_best_only=True)\n",
    "\n",
    "lenet_5_improved_model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), callbacks=[early_stopping_cb])\n",
    "\n",
    "lenet_5_improved_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "lenet_5_selu_dropout_model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(32, kernel_size=5, strides=1,  activation=\"selu\", kernel_initializer=\"lecun_normal\", input_shape=X_train[0].shape, padding='same'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(64, kernel_size=5, strides=1, activation=\"selu\", kernel_initializer=\"lecun_normal\", padding='valid'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(32, kernel_size=3, strides=1, activation=\"selu\", kernel_initializer=\"lecun_normal\", padding='valid'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.GlobalAveragePooling2D(),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(62, activation='softmax')\n",
    "])\n",
    "\n",
    "lenet_5_selu_dropout_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_5_selu_dropout_model.compile(optimizer='adam', loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "# model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_lenet5_emnist_model.h5\", save_best_only=True)\n",
    "  \n",
    "lenet_5_selu_dropout_model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), callbacks=[early_stopping_cb])\n",
    "\n",
    "lenet_5_selu_dropout_model.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "lenet_5_selu_bn_model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(32, kernel_size=5, strides=1,  activation=\"selu\", kernel_initializer=\"lecun_normal\", input_shape=X_train[0].shape, padding='same'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(64, kernel_size=3, strides=1, activation=\"selu\", kernel_initializer=\"lecun_normal\", padding='valid'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(32, kernel_size=3, strides=1, activation=\"selu\", kernel_initializer=\"lecun_normal\", padding='valid'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.GlobalAveragePooling2D(),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(62, activation='softmax')\n",
    "])\n",
    "\n",
    "lenet_5_selu_bn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_5_selu_bn_model.compile(optimizer='adam', loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "# model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_lenet5_emnist_model.h5\", save_best_only=True)\n",
    "  \n",
    "lenet_5_selu_bn_model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), callbacks=[early_stopping_cb])\n",
    "\n",
    "lenet_5_selu_bn_model.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 - B.  Res base - new architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(keras.models.Model):\n",
    "    \"\"\"\n",
    "    A standard resnet block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels: int, down_sample=False):\n",
    "        \"\"\"\n",
    "        channels: same as number of convolution kernels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.__channels = channels\n",
    "        self.__down_sample = down_sample\n",
    "        self.__strides = [2, 1] if down_sample else [1, 1]\n",
    "\n",
    "        KERNEL_SIZE = (3, 3)\n",
    "        # use He initialization, instead of Xavier (a.k.a 'glorot_uniform' in Keras), as suggested in [2]\n",
    "        INIT_SCHEME = \"he_normal\"\n",
    "\n",
    "        self.conv_1 = keras.layers.Conv2D(self.__channels, strides=self.__strides[0],\n",
    "                             kernel_size=KERNEL_SIZE, padding=\"same\", kernel_initializer=INIT_SCHEME)\n",
    "        self.bn_1 = keras.layers.BatchNormalization()\n",
    "        self.conv_2 = keras.layers.Conv2D(self.__channels, strides=self.__strides[1],\n",
    "                             kernel_size=KERNEL_SIZE, padding=\"same\", kernel_initializer=INIT_SCHEME)\n",
    "        self.bn_2 = keras.layers.BatchNormalization()\n",
    "        self.merge = keras.layers.Add()\n",
    "\n",
    "        if self.__down_sample:\n",
    "            # perform down sampling using stride of 2, according to [1].\n",
    "            self.res_conv = keras.layers.Conv2D(\n",
    "                self.__channels, strides=2, kernel_size=(1, 1), kernel_initializer=INIT_SCHEME, padding=\"same\")\n",
    "            self.res_bn = keras.layers.BatchNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        res = inputs\n",
    "\n",
    "        x = self.conv_1(inputs)\n",
    "        x = self.bn_1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.bn_2(x)\n",
    "\n",
    "        if self.__down_sample:\n",
    "            res = self.res_conv(res)\n",
    "            res = self.res_bn(res)\n",
    "\n",
    "        # if not perform down sample, then add a shortcut directly\n",
    "        x = self.merge([x, res])\n",
    "        out = tf.nn.relu(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18(keras.models.Model):\n",
    "\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        \"\"\"\n",
    "            num_classes: number of classes in specific classification task.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv_1 = keras.layers.Conv2D(64, (7, 7), strides=2,\n",
    "                             padding=\"same\", kernel_initializer=\"he_normal\")\n",
    "        self.init_bn = keras.layers.BatchNormalization()\n",
    "        self.pool_2 = keras.layers.MaxPool2D(pool_size=(2, 2), strides=2, padding=\"same\")\n",
    "        self.res_1_1 = ResnetBlock(64)\n",
    "        self.res_1_2 = ResnetBlock(64)\n",
    "        self.res_2_1 = ResnetBlock(128, down_sample=True)\n",
    "        self.res_2_2 = ResnetBlock(128)\n",
    "        self.res_3_1 = ResnetBlock(256, down_sample=True)\n",
    "        self.res_3_2 = ResnetBlock(256)\n",
    "        self.res_4_1 = ResnetBlock(512, down_sample=True)\n",
    "        self.res_4_2 = ResnetBlock(512)\n",
    "        self.avg_pool = keras.layers.GlobalAveragePooling2D()\n",
    "        self.flat = keras.layers.Flatten()\n",
    "        self.fc = keras.layers.Dense(num_classes, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        out = self.conv_1(inputs)\n",
    "        out = self.init_bn(out)\n",
    "        out = tf.nn.relu(out)\n",
    "        out = self.pool_2(out)\n",
    "        for res_block in [self.res_1_1, self.res_1_2, self.res_2_1, self.res_2_2, self.res_3_1, self.res_3_2, self.res_4_1, self.res_4_2]:\n",
    "            out = res_block(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = self.flat(out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = ResNet18(62)\n",
    "resnet_model.build(input_shape = (None,28,28,1))\n",
    "#use categorical_crossentropy since the label is one-hot encoded\n",
    "\n",
    "# from keras.optimizers import SGD\n",
    "# opt = SGD(learning_rate=0.1,momentum=0.9,decay = 1e-04) #parameters suggested by He [1]\n",
    "resnet_model.compile(optimizer = \"adam\",loss='categorical_crossentropy', metrics=[\"accuracy\"]) \n",
    "resnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit input kernel size & seperate upper layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Res_5x5(keras.models.Model):\n",
    "\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        \"\"\"\n",
    "            num_classes: number of classes in specific classification task.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        input_ch = 32\n",
    "        self.conv_1 = keras.layers.Conv2D(input_ch, (5, 5), strides=1,\n",
    "                             padding=\"same\", kernel_initializer=\"he_normal\")\n",
    "        self.init_bn = keras.layers.BatchNormalization()\n",
    "        self.pool_2 = keras.layers.MaxPool2D(pool_size=(2, 2), strides=1, padding=\"same\")\n",
    "        self.res_1_1 = ResnetBlock(input_ch)\n",
    "        self.res_1_2 = ResnetBlock(input_ch)\n",
    "        self.res_2_1 = ResnetBlock(input_ch*2, down_sample=True)\n",
    "        self.res_2_2 = ResnetBlock(input_ch*2)\n",
    "        self.res_3_1 = ResnetBlock(input_ch*4, down_sample=True)\n",
    "        self.res_3_2 = ResnetBlock(input_ch*4)\n",
    "        \n",
    "        self.res_num_1 = ResnetBlock(input_ch*8, down_sample=True)\n",
    "        self.res_num_2 = ResnetBlock(input_ch*8)\n",
    "        \n",
    "        self.res_upper_1 = ResnetBlock(input_ch*8, down_sample=True)\n",
    "        self.res_upper_2 = ResnetBlock(input_ch*8)\n",
    "        self.res_upper_3 = ResnetBlock(input_ch*8)\n",
    "   \n",
    "\n",
    "        self.res_lower_1 = ResnetBlock(input_ch*8, down_sample=True)\n",
    "        self.res_lower_2 = ResnetBlock(input_ch*8)\n",
    "        self.res_lower_3 = ResnetBlock(input_ch*8)\n",
    "\n",
    "        self.num_avg_pool = keras.layers.GlobalAveragePooling2D()\n",
    "        self.upper_avg_pool = keras.layers.GlobalAveragePooling2D()\n",
    "        self.lower_avg_pool = keras.layers.GlobalAveragePooling2D()\n",
    "\n",
    "        self.num_flat = keras.layers.Flatten()\n",
    "        self.upper_flat = keras.layers.Flatten()\n",
    "        self.lower_flat = keras.layers.Flatten()\n",
    "\n",
    "        self.fc_out = keras.layers.Dense(num_classes, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        out = self.conv_1(inputs)\n",
    "        out = self.init_bn(out)\n",
    "        out = tf.nn.relu(out)\n",
    "        out = self.pool_2(out)\n",
    "        for res_block in [self.res_1_1, self.res_1_2, self.res_2_1, self.res_2_2, self.res_3_1, self.res_3_2]:\n",
    "            out = res_block(out)\n",
    "        \n",
    "        num_out = self.res_num_1(out)\n",
    "        num_out = self.res_num_2(num_out)\n",
    "        num_out = self.num_avg_pool(num_out)\n",
    "        num_out = self.num_flat(num_out)\n",
    "\n",
    "        upper_out = self.res_upper_1(out)\n",
    "        upper_out = self.res_upper_2(upper_out)\n",
    "        upper_out = self.res_upper_3(upper_out)\n",
    "        upper_out = self.upper_avg_pool(upper_out)\n",
    "        upper_out = self.upper_flat(upper_out)\n",
    "\n",
    "        lower_out = self.res_lower_1(out)\n",
    "        lower_out = self.res_lower_2(lower_out)\n",
    "        lower_out = self.res_lower_3(lower_out)\n",
    "        lower_out = self.lower_avg_pool(lower_out)\n",
    "        lower_out = self.lower_flat(lower_out)\n",
    "\n",
    "        out = keras.layers.Concatenate()([num_out, upper_out, lower_out])\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_base_model = Res_5x5(62)\n",
    "res_base_model.build(input_shape = (None,28,28,1))\n",
    "#use categorical_crossentropy since the label is one-hot encoded\n",
    "\n",
    "# from keras.optimizers import SGD\n",
    "# opt = SGD(learning_rate=0.1,momentum=0.9,decay = 1e-04) #parameters suggested by He [1]\n",
    "res_base_model.compile(optimizer = \"adam\",loss='categorical_crossentropy', metrics=[\"accuracy\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\",\n",
    ")\n",
    "\n",
    "es = EarlyStopping(patience= 5, restore_best_weights=True, monitor=\"val_accuracy\")\n",
    "\n",
    "batch_size = 256\n",
    "#I did not use cross validation, so the validate performance is not accurate.\n",
    "STEPS = len(X_train) / 256\n",
    "\n",
    "res_base_history = resnet_model.fit(aug.flow(X_train,y_train,batch_size = batch_size), \n",
    "      steps_per_epoch=STEPS, \n",
    "      batch_size = batch_size, \n",
    "      epochs=50, \n",
    "      validation_data=(X_val, y_val),\n",
    "      callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Res + Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Res_dense(keras.models.Model):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "            num_classes: number of classes in specific classification task.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        input_ch = 32\n",
    "        self.conv_1 = keras.layers.Conv2D(input_ch, (5, 5), strides=1,\n",
    "                             padding=\"same\", kernel_initializer=\"he_normal\")\n",
    "        self.BN = keras.layers.BatchNormalization()\n",
    "        self.pool_2 = keras.layers.MaxPool2D(pool_size=(2, 2), strides=1, padding=\"same\")\n",
    "        self.res_1_1 = ResnetBlock(input_ch)\n",
    "        self.res_1_2 = ResnetBlock(input_ch)\n",
    "        self.res_2_1 = ResnetBlock(input_ch*2, down_sample=True)\n",
    "        self.res_2_2 = ResnetBlock(input_ch*2)\n",
    "        self.res_2_1 = ResnetBlock(input_ch*4, down_sample=True)\n",
    "        self.res_2_2 = ResnetBlock(input_ch*4)\n",
    "        \n",
    "        self.avg_pool = keras.layers.GlobalAveragePooling2D()\n",
    "        self.flat = keras.layers.Flatten()\n",
    "        \n",
    "        self.res_num_1 = keras.layers.Dense(46, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "        self.res_num_1 = keras.layers.Dense(28, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "        self.res_num_3 = keras.layers.Dense(10, activation = 'softmax')\n",
    "        \n",
    "        self.res_upper_1 = keras.layers.Dense(40, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "        self.res_upper_2 = keras.layers.Dense(26, activation = 'softmax')\n",
    "   \n",
    "\n",
    "        self.res_lower_1 = keras.layers.Dense(48, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "        self.res_lower_1 = keras.layers.Dense(32, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "        self.res_lower_3 = keras.layers.Dense(26, activation = 'softmax')\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        out = self.BN(inputs)\n",
    "        out = self.conv_1(out)\n",
    "        out = self.BN(out)\n",
    "        out = self.pool_2(out)\n",
    "        for res_block in [self.res_1_1, self.res_1_2, self.res_2_1, self.res_2_2]:\n",
    "            out = res_block(out)\n",
    "        \n",
    "        out = self.avg_pool(out)\n",
    "        out = self.flat(out)\n",
    "\n",
    "        num_out = self.res_num_1(out)\n",
    "        num_out = self.BN(num_out)\n",
    "        num_out = self.res_num_2(num_out)\n",
    "        num_out = self.BN(num_out)\n",
    "        num_out = self.res_num_3(num_out)\n",
    "\n",
    "        upper_out = self.res_upper_1(out)\n",
    "        upper_out = self.BN(upper_out)\n",
    "        upper_out = self.res_upper_2(upper_out)\n",
    "\n",
    "        lower_out = self.res_lower_1(out)\n",
    "        lower_out = self.BN(lower_out)\n",
    "        lower_out = self.res_lower_2(lower_out)\n",
    "        lower_out = self.BN(lower_out)\n",
    "        lower_out = self.res_lower_3(lower_out)\n",
    "\n",
    "        out = keras.layers.Concatenate()([num_out, upper_out, lower_out])\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dense_model = Res_dense()\n",
    "res_dense_model.build(input_shape = (None,28,28,1))\n",
    "#use categorical_crossentropy since the label is one-hot encoded\n",
    "\n",
    "# from keras.optimizers import SGD\n",
    "# opt = SGD(learning_rate=0.1,momentum=0.9,decay = 1e-04) #parameters suggested by He [1]\n",
    "res_dense_model.compile(optimizer = \"adam\",loss='categorical_crossentropy', metrics=[\"accuracy\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\",\n",
    ")\n",
    "\n",
    "es = EarlyStopping(patience= 5, restore_best_weights=True, monitor=\"val_accuracy\")\n",
    "\n",
    "batch_size = 256\n",
    "#I did not use cross validation, so the validate performance is not accurate.\n",
    "STEPS = len(X_train) / 256\n",
    "\n",
    "res_dense_history = resnet_model.fit(aug.flow(X_train,y_train,batch_size = batch_size), \n",
    "      steps_per_epoch=STEPS, \n",
    "      batch_size = batch_size, \n",
    "      epochs=50, \n",
    "      validation_data=(X_val, y_val),\n",
    "      callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 - C.  VGG – pre-trained"
   ]
  },
   {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape = (28,28,1))\n",
    "x = Conv2D (filters =32, kernel_size =3, padding ='same', activation='relu')(inputs)\n",
    "x = Conv2D (filters =32, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x= BatchNormalization()(x)\n",
    "x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "# 2nd Conv Block\n",
    "x = Conv2D (filters =64, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = Conv2D (filters =64, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x= BatchNormalization()(x)\n",
    "x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "# 3rd Conv block  \n",
    "x = Conv2D (filters =128, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = Conv2D (filters =128, kernel_size =3, padding ='same', activation='relu')(x) \n",
    "x = Conv2D (filters =128, kernel_size =3, padding ='same', activation='relu')(x) \n",
    "x= BatchNormalization()(x)\n",
    "x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "# 4th Conv block\n",
    "x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x= BatchNormalization()(x)\n",
    "x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "# 5th Conv block\n",
    "x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x= BatchNormalization()(x)\n",
    "x= Dropout(0.3)(x)\n",
    "x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "# Fully connected layers  \n",
    "x = Flatten()(x)\n",
    "x= BatchNormalization()(x)\n",
    "x = Dense(units = 4096, activation ='relu')(x)\n",
    "x = Dense(units = 4096, activation ='relu')(x)\n",
    "output = Dense(units = 62, activation ='softmax')(x)\n",
    "# creating the model\n",
    "\n",
    "VGG_model = Model (inputs=inputs, outputs =output)\n",
    "VGG_model.summary()"
   ]
  }
 ,
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.005), loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "  \n",
    "VGG_model.fit(X_train, y_train, batch_size = 32, epochs=100, validation_data=(X_val, y_val), callbacks=[early_stopping_cb])\n",
    "\n",
    "VGG_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape = (28,28,1))\n",
    "x = Conv2D (filters =32, kernel_size =3, padding ='same', activation='relu')(inputs)\n",
    "x = Conv2D (filters =32, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x= BatchNormalization()(x)\n",
    "x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "# 2nd Conv Block\n",
    "x = Conv2D (filters =64, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = Conv2D (filters =64, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x= BatchNormalization()(x)\n",
    "x= Dropout(0.3)(x)\n",
    "x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "# 3rd Conv block  \n",
    "x = Conv2D (filters =128, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = Conv2D (filters =128, kernel_size =3, padding ='same', activation='relu')(x) \n",
    "x = Conv2D (filters =128, kernel_size =3, padding ='same', activation='relu')(x) \n",
    "x= BatchNormalization()(x)\n",
    "x= Dropout(0.3)(x)\n",
    "x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "# 4th Conv block\n",
    "x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x= BatchNormalization()(x)\n",
    "x= Dropout(0.3)(x)\n",
    "x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "# 5th Conv block\n",
    "x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x= BatchNormalization()(x)\n",
    "x= Dropout(0.3)(x)\n",
    "x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "# Fully connected layers  \n",
    "x = Flatten()(x)\n",
    "x= BatchNormalization()(x)\n",
    "x = Dense(units = 4096, activation ='relu')(x)\n",
    "x = Dense(units = 4096, activation ='relu')(x)\n",
    "output = Dense(units = 62, activation ='softmax')(x)\n",
    "# creating the model\n",
    "\n",
    "VGG_model_2 = Model (inputs=inputs, outputs =output)\n",
    "VGG_model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_model_2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.005), loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "# model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_new_emnist_model.h5\", save_best_only=True)\n",
    "  \n",
    "VGG_model_2.fit(X_train, y_train, batch_size = 32, epochs=100, validation_data=(X_val, y_val), callbacks=[early_stopping_cb])\n",
    "\n",
    "VGG_model_2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape = (28,28,1))\n",
    "x = Conv2D (filters =32, kernel_size =3, padding ='same', activation='relu')(inputs)\n",
    "x = Conv2D (filters =32, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x= BatchNormalization()(x)\n",
    "x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "# 2nd Conv Block\n",
    "x = Conv2D (filters =64, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = Conv2D (filters =64, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x= BatchNormalization()(x)\n",
    "x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "# 3rd Conv block  \n",
    "x = Conv2D (filters =128, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = Conv2D (filters =128, kernel_size =3, padding ='same', activation='relu')(x) \n",
    "x = Conv2D (filters =128, kernel_size =3, padding ='same', activation='relu')(x) \n",
    "x= BatchNormalization()(x)\n",
    "x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "# 4th Conv block\n",
    "x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x= BatchNormalization()(x)\n",
    "x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "# 5th Conv block\n",
    "x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x= BatchNormalization()(x)\n",
    "x= Dropout(0.3)(x)\n",
    "x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "# Fully connected layers  \n",
    "x = Flatten()(x)\n",
    "x= BatchNormalization()(x)\n",
    "x = Dense(units = 4096, activation ='relu')(x)\n",
    "x = Dense(units = 4096, activation ='relu')(x)\n",
    "output = Dense(units = 62, activation ='softmax')(x)\n",
    "# creating the model\n",
    "\n",
    "VGG_model_3 = Model (inputs=inputs, outputs =output)\n",
    "VGG_model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_model_3.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.01), loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "# model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_new_emnist_model.h5\", save_best_only=True)\n",
    "  \n",
    "VGG_model_3.fit(X_train, y_train, batch_size = 64, epochs=100, validation_data=(X_val, y_val), callbacks=[early_stopping_cb])\n",
    "\n",
    "VGG_model_3.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new VGG\n",
    "inputs = Input(shape = (28,28,1))\n",
    "x = Conv2D (filters =32, kernel_size =3, padding ='same', activation='relu')(inputs)\n",
    "x = Conv2D (filters =32, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "# 2nd Conv Block\n",
    "x = Conv2D (filters =64, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = Conv2D (filters =64, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "# 3rd Conv block  \n",
    "x = Conv2D (filters =128, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = Conv2D (filters =128, kernel_size =3, padding ='same', activation='relu')(x) \n",
    "x = Conv2D (filters =128, kernel_size =3, padding ='same', activation='relu')(x) \n",
    "x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "# 4th Conv block\n",
    "x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "# 5th Conv block\n",
    "x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "# Fully connected layers  \n",
    "x = Flatten()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(units = 4096, activation ='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(units = 4096, activation ='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "output = Dense(units = 62, activation ='softmax')(x)\n",
    "# creating the model\n",
    "\n",
    "VGG_model_4 = Model (inputs=inputs, outputs =output)\n",
    "VGG_model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_model_4.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.005), loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "# model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_new_model.h5\", save_best_only=True)\n",
    "  \n",
    "VGG_model_4.fit(X_train, y_train, batch_size = 64, epochs=100, validation_data=(X_val, y_val), callbacks=[early_stopping_cb])\n",
    "\n",
    "VGG_model_4.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 - D.  Efficient net - pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_learning_curve(history, model_name):\n",
    "  pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "  plt.grid(True)\n",
    "  plt.gca().set_ylim(0, 1)\n",
    "  plt.title(f\"{model_name}'s learnig curve\", fontsize=14)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_crop(data, mean):\n",
    "  # images = np.zeros((data.shape[0], 20, 20))\n",
    "  for i in range(data.shape[0]):\n",
    "    image = data[i]\n",
    "    first_norm = 255/np.max(image)\n",
    "    image = (image*first_norm).astype(np.uint8)\n",
    "\n",
    "    _, binary_image = cv2.threshold(image.astype(np.uint8), 20, 255, cv2.THRESH_BINARY)\n",
    "    min_max_list = np.where(binary_image==255)\n",
    "    x_min, x_max = np.min(min_max_list[1]), np.max(min_max_list[1])\n",
    "    y_min, y_max = np.min(min_max_list[0]), np.max(min_max_list[0])\n",
    "\n",
    "    if abs(x_max - x_min) >abs(y_max - y_min):\n",
    "      add_sub_factor = (abs(x_max - x_min) - abs(y_max - y_min))//2\n",
    "      y_min = max(0, y_min-add_sub_factor-1)\n",
    "      y_max = min(27, y_max+add_sub_factor+1)\n",
    "\n",
    "    if abs(y_max - y_min) > abs(x_max - x_min):\n",
    "      add_sub_factor = (abs(y_max - y_min)-abs(x_max - x_min))//2\n",
    "      x_min = max(0, x_min-add_sub_factor-1)\n",
    "      x_max = min(27, x_max+add_sub_factor+1)\n",
    "    # print(x_min, x_max)\n",
    "    # print(y_min, y_max)\n",
    "    data[i] = np.pad(cv2.resize(image.astype(np.uint8)[y_min:y_max, x_min:x_max], (20, 20)), ((4, 4), (4, 4)), 'constant', constant_values = 0)\n",
    "    second_norm = 255/np.max(data[i])\n",
    "    data[i] = (data[i]*second_norm).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.normalize(nparr, axis=-1, order=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
